import requests
from bs4 import BeautifulSoup
from jupiter import ask_jupiter # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥—É–ª—å


CATEGORIES = {
    "–º–∏—Ä–æ–≤—ã–µ": {"rbc": "/world/", "ria": "/world/"},
    "—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ": {"rbc": "/", "ria": "/russia/"},
    "–≥–æ—Ä–æ–¥—Å–∫–∏–µ": {"rbc": "/tag/–≥–æ—Ä–æ–¥/", "ria": "/region/"},
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": {"rbc": "/economics/", "ria": "/economy/"},
    "–ø–æ–ª–∏—Ç–∏–∫–∞": {"rbc": "/politics/", "ria": "/politics/"},
    "–∞–≤—Ç–æ": {"rbc": "/auto/", "ria": "/auto/"},
    "–Ω–∞—É–∫–∞": {"rbc": "/science_and_technology/", "ria": "/science/"},
    "–±—É–¥–Ω–∏": {"rbc": "/", "ria": "/society/"}
}
API_KEY = '5GbXmau93XwA5QJwtrx5lugncxLUylcf'


def get_news_from_rbc(section_url: str, limit=5):
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ –†–ë–ö"""
    base_url = "https://www.rbc.ru"
    full_url = base_url + section_url
    response = requests.get(full_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.content, "lxml")

    articles = soup.select("a.main__feed__link")
    news = []

    for a in articles[:limit]:
        title = a.get_text(strip=True)
        link = a['href']
        try:
            article_resp = requests.get(link, headers={"User-Agent": "Mozilla/5.0"})
            article_soup = BeautifulSoup(article_resp.content, "lxml")
            body = article_soup.select_one("div.article__text, div.article__content")
            text = body.get_text(" ", strip=True) if body else ""
            if text:
                news.append(text[:500])
        except Exception:
            continue
    return news


def get_news_from_ria(section_url: str, limit=5):
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ –†–ò–ê"""
    base_url = "https://ria.ru"
    full_url = base_url + section_url
    response = requests.get(full_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.content, "lxml")

    articles = soup.select("a.list-item__title, a.cell-main-photo__link")
    news = []

    for a in articles[:limit]:
        title = a.get_text(strip=True)
        href = a['href']
        link = href if href.startswith("http") else base_url + href
        try:
            article_resp = requests.get(link, headers={"User-Agent": "Mozilla/5.0"})
            article_soup = BeautifulSoup(article_resp.content, "lxml")
            content_div = article_soup.select_one("div.article__body, div.article__text, div.article")
            text = content_div.get_text(" ", strip=True) if content_div else ""
            if text:
                news.append(text[:500])
        except Exception:
            continue
    return news


def fetch_news(topic: str, count=5):
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏"""
    topic = topic.lower()
    category = CATEGORIES.get(topic, {"rbc": "/", "ria": "/"})

    print(f"üì° –ü–æ–ª—É—á–∞—é –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–µ: ¬´{topic}¬ª")

    rbc_news = get_news_from_rbc(category["rbc"], limit=count)
    ria_news = get_news_from_ria(category["ria"], limit=count)

    all_news = rbc_news + ria_news
    return "\n".join(all_news[:count])


def get_traffic_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ—Ä–æ–∂–Ω–æ–π –æ–±—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    api_url = "https://api.codd.mos.ru/traffic"

    try:
        response = requests.get(api_url)
        response.raise_for_status()
        data = response.json()

        for item in data['features']:
            properties = item['properties']
            print(f"–£—á–∞—Å—Ç–æ–∫: {properties['name']}")
            print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: {properties['status']}")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {properties['description']}\n")

    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
def get_traffic_info(city="–ú–æ—Å–∫–≤–∞"):
    """
    –ü–∞—Ä—Å–∏—Ç –¥–æ—Ä–æ–∂–Ω—É—é –æ–±—Å—Ç–∞–Ω–æ–≤–∫—É —Å ria.ru –¥–ª—è –ú–æ—Å–∫–≤—ã –∏ –¥–∞—ë—Ç –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –Æ–ø–∏—Ç–µ—Ä–∞.
    """
    print(f"üöó –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –≥–æ—Ä–æ–¥–∞: {city}")
    traffic_url = "https://ria.ru/traffic/"

    try:
        response = requests.get(traffic_url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.content, "lxml")

        articles = soup.select("a.list-item__title")
        traffic_news = []

        for a in articles[:3]:  # –±–µ—Ä—ë–º 3 –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏
            title = a.get_text(strip=True)
            href = a['href']
            full_url = href if href.startswith("http") else "https://ria.ru" + href

            art_resp = requests.get(full_url, headers={"User-Agent": "Mozilla/5.0"})
            art_soup = BeautifulSoup(art_resp.content, "lxml")
            body = art_soup.select_one("div.article__text, div.article__body")
            if body:
                text = body.get_text(" ", strip=True)
                if city.lower() in text.lower():
                    traffic_news.append(text[:500])

        if not traffic_news:
            return "–î–∞–Ω–Ω—ã—Ö –ø–æ –¥–æ—Ä–æ–∂–Ω–æ–π –æ–±—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–æ–∫–∞ –Ω–µ—Ç."

        prompt = (
            f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Å–∏—Ç—É–∞—Ü–∏–∏ –Ω–∞ –¥–æ—Ä–æ–≥–∞—Ö –≤ –≥–æ—Ä–æ–¥–µ {city} –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:\n"
            + "\n\n".join(traffic_news)
        )

        return ask_jupiter(prompt)

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ —Ç—Ä–∞—Ñ–∏–∫–µ: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–∫–∞—Ö."


def main(topic, count, city):
    topic = topic.strip().lower()
    count_input = count

    try:
        count = int(count_input)
    except ValueError:
        count = 5

    if topic == "—Ç—Ä–∞—Ñ–∏–∫":
        city = city.strip() or "–ú–æ—Å–∫–≤–∞"
        summary = get_traffic_info(city)
        print("üì¢ –Æ–ø–∏—Ç–µ—Ä:\n")
        print(summary)
        return

    raw_news = fetch_news(topic, count)
    if not raw_news:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏.")
        return


    combined_text = "\n\n".join(raw_news)
    prompt = f"–°–¥–µ–ª–∞–π –∂–∏–≤—É—é –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ–º—É: {topic}. –í–æ—Ç —Ç–µ–∫—Å—Ç—ã:\n{combined_text}"

    print("\nüß† –Æ–ø–∏—Ç–µ—Ä –≥–æ—Ç–æ–≤–∏—Ç —Å–≤–æ–¥–∫—É...\n")
    summary = ask_jupiter(prompt)
    print("üì¢ –Æ–ø–∏—Ç–µ—Ä:\n")
    return summary

def get_traffic_news():
    headers = {'User-Agent': 'Mozilla/5.0'}
    news_data = []

    rbc_url = 'https://www.rbc.ru/tags/?tag=–ø—Ä–æ–±–∫–∏'
    rbc_response = requests.get(rbc_url, headers=headers)
    if rbc_response.status_code == 200:
        rbc_soup = BeautifulSoup(rbc_response.text, 'html.parser')
        rbc_articles = rbc_soup.find_all('a', class_='item__link', limit=5)
        for article in rbc_articles:
            title = article.get_text(strip=True)
            link = article['href']
            news_data.append(f"{title} ({link})")

    ria_url = 'https://ria.ru/tag_traffic_jam/'
    ria_response = requests.get(ria_url, headers=headers)
    if ria_response.status_code == 200:
        ria_soup = BeautifulSoup(ria_response.text, 'html.parser')
        ria_articles = ria_soup.find_all('a', class_='list-item__title', limit=5)
        for article in ria_articles:
            title = article.get_text(strip=True)
            link = article['href']
            news_data.append(f"{title} ({link})")

    return news_data


def get_traffic_flow():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —É–ª–∏—Ü—ã –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ —á–µ—Ä–µ–∑ TomTom API."""
    city_lat, city_lon = 55.0084, 82.9357
    url = f'https://api.tomtom.com/traffic/services/4/flowSegmentData/relative0/10/json?point={city_lat}%2C{city_lon}&key={API_KEY}'

    try:
        response = requests.get(url)

        if response.status_code != 200:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ HTTP {response.status_code}: {response.text}")
            return None

        data = response.json()
        if not isinstance(data, dict) or "flowSegmentData" not in data:
            print("‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            return None

        segment = data["flowSegmentData"]
        current_speed = segment.get("currentSpeed", 0)
        free_flow_speed = segment.get("freeFlowSpeed", 0)
        confidence = int(segment.get("confidence", 0) * 100)

        congestion_level = free_flow_speed - current_speed
        status = "üö¶ –ó–∞—Ç—Ä—É–¥–Ω—ë–Ω–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ" if congestion_level > 10 else "‚úÖ –î–≤–∏–∂–µ–Ω–∏–µ —Å–≤–æ–±–æ–¥–Ω–æ–µ"

        report = (
            f"–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –≤ –≥–æ—Ä–æ–¥–µ: {current_speed} –∫–º/—á\n"
            f"–û–±—ã—á–Ω–æ —Å–≤–æ–±–æ–¥–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ: {free_flow_speed} –∫–º/—á\n"
            f"–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {confidence}%\n"
            f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: {status}"
        )

        return report

    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ API: {e}")

    return "üöó –ù–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –¥–≤–∏–∂–µ–Ω–∏–∏."


def get_traffic_incidents():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–æ—Ä–æ–∂–Ω—ã—Ö –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞—Ö –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ."""
    bbox = "82.5,54.8,83.4,55.4"
    url = f'https://api.tomtom.com/traffic/services/5/incidentDetails?bbox={bbox}&key={API_KEY}'

    try:
        response = requests.get(url)

        if response.status_code != 200:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ HTTP {response.status_code}: {response.text}")
            return None

        data = response.json()
        if not isinstance(data, dict) or "incidents" not in data:
            print("‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            return None

        incidents = [
            {
                "description": i['properties'].get('description', '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è'),
                "start": i['properties'].get('startTime', ''),
                "end": i['properties'].get('endTime', '')
            }
            for i in data.get('incidents', [])
        ]

        return incidents if incidents else "üöó –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è—Ö."

    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ API: {e}")

    return "üöó –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ—Ä–æ–∂–Ω—ã—Ö –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞—Ö."


def summarize_traffic():
    """–°–æ–∑–¥–∞—ë—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –¥–æ—Ä–æ–∂–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å."""
    flow = get_traffic_flow()
    incidents = get_traffic_incidents()

    if not flow:
        flow_text = "–ù–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –¥–≤–∏–∂–µ–Ω–∏–∏."
    else:
        flow_text = flow

    if incidents and isinstance(incidents, list):
        incident_text = "\n".join([f"{i['description']}" for i in incidents[:5]])
    else:
        incident_text = "–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è—Ö."

    combined_data = f"–û–±–∑–æ—Ä —Ç—Ä–∞—Ñ–∏–∫–∞: {flow_text}\n–ò–Ω—Ü–∏–¥–µ–Ω—Ç—ã: {incident_text}"

    return ask_jupiter(f"–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Æ–Ω–æ–Ω–∞(–ó–∞–±—É–¥—å –æ —á–∞—Å—Ç–∏ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –Æ–ø–∏—Ç–µ—Ä–∞), —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –¥–æ—Ä–æ–∂–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏. "
                       f"–ò—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –Ω–∏–∂–µ, —Å—Ñ–æ—Ä–º–∏—Ä—É–π –∫—Ä–∞—Ç–∫—É—é, –ø–æ–ª–µ–∑–Ω—É—é —Å–≤–æ–¥–∫—É –ø—Ä–æ–±–æ–∫ –∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤. "
                       f"–¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—ä–µ–∑–¥—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤: {combined_data}")



if __name__ == "__main__":
    print(summarize_traffic())
    main()
